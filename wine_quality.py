# -*- coding: utf-8 -*-
"""Wine_Quality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ELsd45iSNBuUw5pcI3U2v1XsMC9yTooQ

Performance Evaluation and Optimization of Supervised Machine Learning Models for Wine Quality Classification

STEP 1: Import Required Libraries
"""

# STEP 1: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# Preprocessing and evaluation
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc)

"""STEP 2: Load and Explore the Dataset"""

data = pd.read_csv('/content/WineData.csv')   # adjust the path if needed
data.head()

#Data Information: Purpose: Understand dataset size, variable types, and any missing data.
data.info()

#Data Quality
data ['quality'].unique()

#describe
data.describe()

"""STEP 3: Exploratory Data Analysis (EDA)"""

#STEP 3: Exploratory Data Analysis (EDA): This helps you understand class balance and which features might be influential.
# Visualize target variable distribution
sns.countplot(x='quality', data=data)
plt.title('Distribution of Wine Quality')
# Correlation heatmap
plt.figure(figsize=(10,8))
sns.heatmap(data.corr(), cmap='coolwarm', annot=False)
plt.title('Feature Correlation Heatmap')
plt.show()

"""STEP 4: Data Preprocessing"""

# STEP 4: Data Preprocessing
# Define features (X) and target/Output (y)
X = data.drop('quality', axis=1)
y = data['quality']

# Convert to binary classification (example: good wine â‰¥7)
y = np.where(y >= 7, 1, 0)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Feature scaling: caling is crucial for SVM and KNN; target encoding simplifies multiclass labels for metrics like AUC
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""STEP 5: Train Base Models"""

# STEP 5: Train Base Models: This gives you baseline results before optimization.
models ={
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Random Forest": RandomForestClassifier(random_state=42),
    "SVM": SVC(probability=True, random_state=42),
    "KNN": KNeighborsClassifier()
}
# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:,1]

    print(f"===== {name} =====")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1-Score:", f1_score(y_test, y_pred))
    print("ROC-AUC:", roc_auc_score(y_test, y_proba))
    print("\n")



"""STEP 6: Hyperparameter Optimization (GridSearchCV)"""

# STEP 6: Hyperparameter Tuning

# STEP 6: Hyperparameter Tuning for All Models

# Logistic Regression
log_params = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2', 'none'],
    'solver': ['lbfgs', 'liblinear']
}
log_grid = GridSearchCV(LogisticRegression(max_iter=1000),
                        log_params, cv=5, scoring='f1', n_jobs=-1)
log_grid.fit(X_train, y_train)
best_log = log_grid.best_estimator_

# Random Forest
rf_params = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42),
                       rf_params, cv=5, scoring='f1', n_jobs=-1)
rf_grid.fit(X_train, y_train)
best_rf = rf_grid.best_estimator_

# SVM
svm_params = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
svm_grid = GridSearchCV(SVC(probability=True, random_state=42),
                        svm_params, cv=5, scoring='f1', n_jobs=-1)
svm_grid.fit(X_train, y_train)
best_svm = svm_grid.best_estimator_

# KNN
knn_params = {'n_neighbors': [3,5,7,9], 'weights': ['uniform', 'distance']}
knn_grid = GridSearchCV(KNeighborsClassifier(),
                        knn_params, cv=5, scoring='f1', n_jobs=-1)
knn_grid.fit(X_train, y_train)
best_knn = knn_grid.best_estimator_

"""STEP 7: Evaluate Optimized Models"""

# STEP 7: Evaluate Optimized Models
optimized_models = {
    "Logistic Regression (Tuned)": best_log,
    "Random Forest (Tuned)": best_rf,
    "SVM (Tuned)": best_svm,
    "KNN (Tuned)": best_knn
}

for name, model in optimized_models.items():
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:,1]

    print(f"===== {name} =====")
    print("Accuracy:", round(accuracy_score(y_test, y_pred), 4))
    print("Precision:", round(precision_score(y_test, y_pred), 4))
    print("Recall:", round(recall_score(y_test, y_pred), 4))
    print("F1-Score:", round(f1_score(y_test, y_pred), 4))
    print("ROC-AUC:", round(roc_auc_score(y_test, y_proba), 4))
    print(classification_report(y_test, y_pred))
    print("\n")

"""STEP 8: ROC Curves Visualization"""

# STEP 8: ROC Curve Comparison
plt.figure(figsize=(8,6))
for name, model in optimized_models.items():
    y_proba = model.predict_proba(X_test)[:,1]
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc(fpr, tpr):.3f})')

plt.plot([0,1],[0,1],'--',color='gray')
plt.title('ROC Curves for Tuned Models')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()

"""STEP 9: Comparative Results Table"""

# STEP 9: Comparison Summary Table

results = []
for name, model in optimized_models.items():
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:,1]
    results.append({
        'Model': name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred),
        'ROC-AUC': roc_auc_score(y_test, y_proba)
    })

results_df = pd.DataFrame(results)
results_df.sort_values(by='F1-Score', ascending=False)